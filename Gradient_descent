###### THIS CODE IS FOR PREDICTING A FUNCTION FROM A DATA SET WITH GRADIENT DECENT
import numpy as np

def gradient_descent(x,y):
    m_curr=b_curr=0
    iterations=10000
    n=len(x)
    learning_rate=0.0002  ## This value will be changed and you have to fine tune this value according to the cost function values. If the cost values are reducing then its ok...otherwise change the learning rate and number of iterations
    for i in range(iterations):
        y_predicted = m_curr*x + b_curr
        cost = (1/n)*sum([val**2 for val in (y-y_predicted)])
        md =-(2/n)*sum(x*(y-y_predicted))# derivative of m wrt x
        bd =-(2/n)*sum(y-y_predicted)    # derivative of b wrt y
        m_curr = m_curr-learning_rate*md  # reducing the steps while getting near the global minimum
        b_curr = b_curr-learning_rate*bd
        print("m_curr {}, b_curr {}, cost {} iterations {} y_predicted {}".format(m_curr,b_curr,cost,i,y_predicted))

x=np.array([92,56,88,70,80,49,65,35,66,67])
y=np.array([98,68,81,80,83,52,66,30,68,73])


gradient_descent(x,y)

output: # last output values are shown here for less complexity
m_curr 1.0390564126680653, b_curr 0.4043138636476907, cost 31.735565790884426 iterations 9991 y_predicted [95.99751417 58.5914657  91.84128656 73.13826233 83.52883135 51.31806738
 67.94297782 36.77127075 68.98203472 70.02109162]
m_curr 1.0390559232031507, b_curr 0.4043485507557409, cost 31.735559773502015 iterations 9992 y_predicted [95.99750383 58.59147297 91.84127818 73.13826275 83.52882688 51.31807808
 67.94298069 36.77128831 68.9820371  70.02109351]
m_curr 1.039055433749471, b_curr 0.4043832370674504, cost 31.735553756395866 iterations 9993 y_predicted [95.99749349 58.59148025 91.84126979 73.13826317 83.52882241 51.31808879
 67.94298356 36.77130586 68.98203948 70.02109541]
m_curr 1.0390549443070305, b_curr 0.40441792258283754, cost 31.735547739566016 iterations 9994 y_predicted [95.99748314 58.59148753 91.84126141 73.1382636  83.52881794 51.31809949
 67.94298643 36.77132342 68.98204186 70.0210973 ]
m_curr 1.039054454875824, b_curr 0.40445260730192056, cost 31.735541723012403 iterations 9995 y_predicted [95.9974728  58.5914948  91.84125302 73.13826402 83.52881347 51.31811019
 67.9429893  36.77134097 68.98204425 70.02109919]
m_curr 1.0390539654558564, b_curr 0.40448729122471777, cost 31.735535706735078 iterations 9996 y_predicted [95.99746246 58.59150208 91.84124464 73.13826445 83.528809   51.3181209
 67.94299217 36.77135853 68.98204663 70.02110108]
m_curr 1.0390534760471224, b_curr 0.4045219743512474, cost 31.735529690733973 iterations 9997 y_predicted [95.99745211 58.59150936 91.84123625 73.13826487 83.52880453 51.3181316
 67.94299505 36.77137608 68.98204901 70.02110298]
m_curr 1.0390529866496263, b_curr 0.4045566566815278, cost 31.735523675009098 iterations 9998 y_predicted [95.99744177 58.59151663 91.84122787 73.1382653  83.52880006 51.3181423
 67.94299792 36.77139364 68.98205139 70.02110487]
m_curr 1.0390524972633637, b_curr 0.4045913382155772, cost 31.735517659560408 iterations 9999 y_predicted [95.99743143 58.59152391 91.84121948 73.13826572 83.52879559 51.318153
 67.94300079 36.77141119 68.98205378 70.02110676]

Process finished with exit code 0
